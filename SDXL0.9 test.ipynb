{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#@title Environment Setup\n\nfrom pathlib import Path\n\nOPTIONS = {}\n\nUPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\nWORKSPACE = '/kaggle/working/ComfyUI'\nOPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n\n%cd /kaggle/working/\n\n![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n%cd $WORKSPACE\n\nif OPTIONS['UPDATE_COMFY_UI']:\n  !echo -= Updating ComfyUI =-\n  !git pull\n\n!echo -= Install dependencies =-\n!pip install xformers -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117\n%env TF_CPP_MIN_LOG_LEVEL=1\n\n!apt -y update -qq\n!wget http://launchpadlibrarian.net/367274644/libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb\n!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/google-perftools_2.5-2.2ubuntu3_all.deb\n!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb\n!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb\n!apt -y install -qq libunwind8-dev\n!dpkg -i *.deb\n%env LD_PRELOAD=libtcmalloc.so\n!rm *.deb","metadata":{"execution":{"iopub.status.busy":"2023-07-08T04:36:40.618431Z","iopub.execute_input":"2023-07-08T04:36:40.618712Z","iopub.status.idle":"2023-07-08T04:39:58.253730Z","shell.execute_reply.started":"2023-07-08T04:36:40.618687Z","shell.execute_reply":"2023-07-08T04:39:58.252303Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working\n-= Initial setup ComfyUI =-\nCloning into 'ComfyUI'...\nremote: Enumerating objects: 5636, done.\u001b[K\nremote: Counting objects: 100% (1841/1841), done.\u001b[K\nremote: Compressing objects: 100% (340/340), done.\u001b[K\nremote: Total 5636 (delta 1626), reused 1532 (delta 1497), pack-reused 3795\u001b[K\nReceiving objects: 100% (5636/5636), 2.74 MiB | 13.58 MiB/s, done.\nResolving deltas: 100% (3682/3682), done.\n/kaggle/working/ComfyUI\n-= Updating ComfyUI =-\nAlready up to date.\n-= Install dependencies =-\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118, https://download.pytorch.org/whl/cu117\nCollecting xformers\n  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.0.0)\nCollecting torchdiffeq (from -r requirements.txt (line 2))\n  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\nCollecting torchsde (from -r requirements.txt (line 3))\n  Downloading torchsde-0.2.5-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting einops (from -r requirements.txt (line 4))\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.30.1)\nRequirement already satisfied: safetensors>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.8.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.12.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (5.4.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (9.5.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.10.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers) (1.23.5)\nCollecting pyre-extensions==0.0.29 (from xformers)\n  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\nCollecting torch (from -r requirements.txt (line 1))\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m445.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\nCollecting triton==2.0.0 (from torch->-r requirements.txt (line 1))\n  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-inspect in /opt/conda/lib/python3.10/site-packages (from pyre-extensions==0.0.29->xformers) (0.9.0)\nCollecting cmake (from triton==2.0.0->torch->-r requirements.txt (line 1))\n  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch->-r requirements.txt (line 1))\n  Downloading lit-16.0.6.tar.gz (153 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: boltons>=20.2.1 in /opt/conda/lib/python3.10/site-packages (from torchsde->-r requirements.txt (line 3)) (23.0.0)\nCollecting trampoline>=0.1.2 (from torchsde->-r requirements.txt (line 3))\n  Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (0.15.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (0.13.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 7)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 7)) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 7)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 7)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 7)) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 7)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 7)) (1.3.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 8)) (5.9.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.25.1->-r requirements.txt (line 5)) (2023.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.25.1->-r requirements.txt (line 5)) (3.0.9)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp->-r requirements.txt (line 7)) (3.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers) (1.0.0)\nBuilding wheels for collected packages: lit\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93582 sha256=2a9b0f5e4d78691b2435ab1320fdfac0c810c615f5e02b15cd41b85e02acf6e9\n  Stored in directory: /root/.cache/pip/wheels/14/f9/07/bb2308587bc2f57158f905a2325f6a89a2befa7437b2d7e137\nSuccessfully built lit\nInstalling collected packages: trampoline, lit, cmake, einops, pyre-extensions, triton, torch, xformers, torchsde, torchdiffeq\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0\n    Uninstalling torch-2.0.0:\n      Successfully uninstalled torch-2.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.26.4 einops-0.6.1 lit-16.0.6 pyre-extensions-0.0.29 torch-2.0.1+cu118 torchdiffeq-0.2.3 torchsde-0.2.5 trampoline-0.1.2 triton-2.0.0 xformers-0.0.20\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0menv: TF_CPP_MIN_LOG_LEVEL=1\n84 packages can be upgraded. Run 'apt list --upgradable' to see them.\n\u001b[1;33mW: \u001b[0mhttp://packages.cloud.google.com/apt/dists/gcsfuse-focal/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n\u001b[1;33mW: \u001b[0mhttps://packages.cloud.google.com/apt/dists/google-fast-socket/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n--2023-07-08 04:39:40--  http://launchpadlibrarian.net/367274644/libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb\nResolving launchpadlibrarian.net (launchpadlibrarian.net)... 185.125.189.228, 185.125.189.229, 2620:2d:4000:1001::8008, ...\nConnecting to launchpadlibrarian.net (launchpadlibrarian.net)|185.125.189.228|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 203696 (199K) [application/x-debian-package]\nSaving to: ‘libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb’\n\nlibgoogle-perftools 100%[===================>] 198.92K   452KB/s    in 0.4s    \n\n2023-07-08 04:39:41 (452 KB/s) - ‘libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb’ saved [203696/203696]\n\n--2023-07-08 04:39:42--  https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/google-perftools_2.5-2.2ubuntu3_all.deb\nResolving launchpad.net (launchpad.net)... 185.125.189.223, 185.125.189.222, 2620:2d:4000:1001::8003, ...\nConnecting to launchpad.net (launchpad.net)|185.125.189.223|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://launchpadlibrarian.net/367274642/google-perftools_2.5-2.2ubuntu3_all.deb [following]\n--2023-07-08 04:39:43--  https://launchpadlibrarian.net/367274642/google-perftools_2.5-2.2ubuntu3_all.deb\nResolving launchpadlibrarian.net (launchpadlibrarian.net)... 185.125.189.229, 185.125.189.228, 2620:2d:4000:1001::8007, ...\nConnecting to launchpadlibrarian.net (launchpadlibrarian.net)|185.125.189.229|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 294224 (287K) [application/x-debian-package]\nSaving to: ‘google-perftools_2.5-2.2ubuntu3_all.deb’\n\ngoogle-perftools_2. 100%[===================>] 287.33K   632KB/s    in 0.5s    \n\n2023-07-08 04:39:44 (632 KB/s) - ‘google-perftools_2.5-2.2ubuntu3_all.deb’ saved [294224/294224]\n\n--2023-07-08 04:39:44--  https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb\nResolving launchpad.net (launchpad.net)... 185.125.189.223, 185.125.189.222, 2620:2d:4000:1001::8003, ...\nConnecting to launchpad.net (launchpad.net)|185.125.189.223|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://launchpadlibrarian.net/367274648/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb [following]\n--2023-07-08 04:39:45--  https://launchpadlibrarian.net/367274648/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb\nResolving launchpadlibrarian.net (launchpadlibrarian.net)... 185.125.189.229, 185.125.189.228, 2620:2d:4000:1001::8007, ...\nConnecting to launchpadlibrarian.net (launchpadlibrarian.net)|185.125.189.229|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 91644 (89K) [application/x-debian-package]\nSaving to: ‘libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb’\n\nlibtcmalloc-minimal 100%[===================>]  89.50K   364KB/s    in 0.2s    \n\n2023-07-08 04:39:46 (364 KB/s) - ‘libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb’ saved [91644/91644]\n\n--2023-07-08 04:39:47--  https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb\nResolving launchpad.net (launchpad.net)... 185.125.189.223, 185.125.189.222, 2620:2d:4000:1001::8003, ...\nConnecting to launchpad.net (launchpad.net)|185.125.189.223|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://launchpadlibrarian.net/367274647/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb [following]\n--2023-07-08 04:39:47--  https://launchpadlibrarian.net/367274647/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb\nResolving launchpadlibrarian.net (launchpadlibrarian.net)... 185.125.189.229, 185.125.189.228, 2620:2d:4000:1001::8008, ...\nConnecting to launchpadlibrarian.net (launchpadlibrarian.net)|185.125.189.229|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 189628 (185K) [application/x-debian-package]\nSaving to: ‘libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb’\n\nlibgoogle-perftools 100%[===================>] 185.18K   455KB/s    in 0.4s    \n\n2023-07-08 04:39:48 (455 KB/s) - ‘libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb’ saved [189628/189628]\n\nThe following additional packages will be installed:\n  liblzma-dev\nSuggested packages:\n  liblzma-doc\nThe following NEW packages will be installed:\n  liblzma-dev libunwind-dev\n0 upgraded, 2 newly installed, 0 to remove and 84 not upgraded.\nNeed to get 2040 kB of archives.\nAfter this operation, 6754 kB of additional disk space will be used.\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package liblzma-dev:amd64.\n(Reading database ... 115376 files and directories currently installed.)\nPreparing to unpack .../liblzma-dev_5.2.5-2ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking liblzma-dev:amd64 (5.2.5-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libunwind-dev:amd64.\nPreparing to unpack .../libunwind-dev_1.3.2-2build2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libunwind-dev:amd64 (1.3.2-2build2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Setting up liblzma-dev:amd64 (5.2.5-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libunwind-dev:amd64 (1.3.2-2build2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Processing triggers for man-db (2.10.2-1) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JSelecting previously unselected package google-perftools.\n(Reading database ... 115469 files and directories currently installed.)\nPreparing to unpack google-perftools_2.5-2.2ubuntu3_all.deb ...\nUnpacking google-perftools (2.5-2.2ubuntu3) ...\nSelecting previously unselected package libgoogle-perftools-dev.\nPreparing to unpack libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb ...\nUnpacking libgoogle-perftools-dev (2.5-2.2ubuntu3) ...\nSelecting previously unselected package libgoogle-perftools4.\nPreparing to unpack libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb ...\nUnpacking libgoogle-perftools4 (2.5-2.2ubuntu3) ...\nSelecting previously unselected package libtcmalloc-minimal4.\nPreparing to unpack libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb ...\nUnpacking libtcmalloc-minimal4 (2.5-2.2ubuntu3) ...\nSetting up libtcmalloc-minimal4 (2.5-2.2ubuntu3) ...\nSetting up libgoogle-perftools4 (2.5-2.2ubuntu3) ...\nSetting up google-perftools (2.5-2.2ubuntu3) ...\nSetting up libgoogle-perftools-dev (2.5-2.2ubuntu3) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.1) ...\nenv: LD_PRELOAD=libtcmalloc.so\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checkpoints\n\n# SD1\n# !wget -c https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt -P ./models/checkpoints/\n!apt-get update\n!apt-get -y install aria2\nprint('正在下载sdxl基本模型，大小12GiB')\n!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://cdn-lfs.huggingface.co/repos/89/5d/895d6e86823f5538cac66a4372bd303af8d5b2e45a341d8eafdd5670c7ceba66/1f697312617db511045698dbf419ae1e2999427d4e4423a321b461cc180d1a97?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sd_xl_base_0.9.safetensors%3B+filename%3D%22sd_xl_base_0.9.safetensors%22%3B&Expires=1689042414&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTA0MjQxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84OS81ZC84OTVkNmU4NjgyM2Y1NTM4Y2FjNjZhNDM3MmJkMzAzYWY4ZDViMmU0NWEzNDFkOGVhZmRkNTY3MGM3Y2ViYTY2LzFmNjk3MzEyNjE3ZGI1MTEwNDU2OThkYmY0MTlhZTFlMjk5OTQyN2Q0ZTQ0MjNhMzIxYjQ2MWNjMTgwZDFhOTc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=j1mzXsC4e3iHeEF14ZfHRNb7ktEfjjMuc6ymvvSgMibyNvE74SD7sfEiZ49BYATgMmEQgk4FjuFp4ZKuQ9OXPZQaflhBiA%7EnieNqAiwoHCMp2bxIw2sEpwgjXJfpKovODy8GJUcDoIFd9H7wGNMrdTN0EzPs3UwDPUAxEga7oicZUaMNifHEh-nyb-oLSRQVD7bx%7EsFHDk5J%7Ev8RprQpKVinlKSPkwOXbgRB5LAh8w8-8KE7ZJf5BalCvbEWHVQdmkxT3n9sMem5gCJ4P-56MsAuLz2IF%7EAEEQKldpH6KSip80staZXRsKK--hkgAmC36votBYVdfWrb7qnL60POhQ__&Key-Pair-Id=KVTP0A1DKRTAX\"  -d /kaggle/working/ComfyUI/models/checkpoints/ -o sd_xl_base_0.9.safetensors\nprint('下载完成')\n# SD2\n#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors -P ./models/checkpoints/\n#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors -P ./models/checkpoints/\n\n# Some SD1.5 anime style\n#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors -P ./models/checkpoints/\n#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors -P ./models/checkpoints/\n#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors -P ./models/checkpoints/\n#!wget -c https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors -P ./models/checkpoints/\n\n# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n#!wget -c https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-fp16.safetensors -P ./models/checkpoints/\n\n\n# unCLIP models\n#!wget -c https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors -P ./models/checkpoints/\n#!wget -c https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors -P ./models/checkpoints/\n\n\n# VAE\n# !wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -P ./models/vae/\n#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -P ./models/vae/\n#!wget -c https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -P ./models/vae/\n\n\n# Loras\n#!wget -c https://civitai.com/api/download/models/10350 -O ./models/loras/theovercomer8sContrastFix_sd21768.safetensors #theovercomer8sContrastFix SD2.x 768-v\n#!wget -c https://civitai.com/api/download/models/10638 -O ./models/loras/theovercomer8sContrastFix_sd15.safetensors #theovercomer8sContrastFix SD1.x\n\n\n# T2I-Adapter\n#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth -P ./models/controlnet/\n#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth -P ./models/controlnet/\n#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth -P ./models/controlnet/\n#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth -P ./models/controlnet/\n#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth -P ./models/controlnet/\n#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth -P ./models/controlnet/\n#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth -P ./models/controlnet/\n\n# T2I Styles Model\n#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth -P ./models/style_models/\n\n# CLIPVision model (needed for styles model)\n#!wget -c https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin -O ./models/clip_vision/clip_vit14.bin\n\n\n# ControlNet\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors -P ./models/controlnet/\n#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors -P ./models/controlnet/\n\n\n# Controlnet Preprocessor nodes by Fannovel16\n#!cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n\n\n# GLIGEN\n#!wget -c https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors -P ./models/gligen/\n\n\n# ESRGAN upscale model\n#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth -P ./models/upscale_models/\n#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -P ./models/upscale_models/\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:03:14.052815Z","iopub.execute_input":"2023-07-08T05:03:14.053261Z","iopub.status.idle":"2023-07-08T05:04:22.074390Z","shell.execute_reply.started":"2023-07-08T05:03:14.053222Z","shell.execute_reply":"2023-07-08T05:04:22.073154Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Hit:1 http://packages.cloud.google.com/apt gcsfuse-focal InRelease\nHit:2 https://packages.cloud.google.com/apt cloud-sdk InRelease                \nHit:3 https://packages.cloud.google.com/apt google-fast-socket InRelease       \nHit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease               \nHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\nHit:6 http://archive.ubuntu.com/ubuntu jammy InRelease                      \nHit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nGet:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [108 kB]\nFetched 108 kB in 2s (60.5 kB/s)   \nReading package lists... Done\nW: http://packages.cloud.google.com/apt/dists/gcsfuse-focal/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\nW: https://packages.cloud.google.com/apt/dists/google-fast-socket/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\naria2 is already the newest version (1.36.0-1).\n0 upgraded, 0 newly installed, 0 to remove and 88 not upgraded.\n/bin/bash: -c: line 1: syntax error near unexpected token `'正在下载sdxl基本模型，大小12GiB''\n/bin/bash: -c: line 1: `print('正在下载sdxl基本模型，大小12GiB')'\n\u001b[35m[\u001b[0m#7acea8 12GiB/12GiB\u001b[36m(98%)\u001b[0m CN:16 DL:\u001b[32m270MiB\u001b[0m\u001b[35m]\u001b[0m\u001b[0m0m\u001b[35m]\u001b[0m\u001b[0mmm\nDownload Results:\ngid   |stat|avg speed  |path/URI\n======+====+===========+=======================================================\n7acea8|\u001b[1;32mOK\u001b[0m  |   257MiB/s|/kaggle/working/ComfyUI/models/checkpoints//sd_xl_base_0.9.safetensors\n\nStatus Legend:\n(OK):download completed.\n","output_type":"stream"}]},{"cell_type":"code","source":"ngrok = False\ndef ngrok():\n    !pip install pyngrok\n    %cd /kaggle/working/hello-ngrok\n    from pyngrok import conf, ngrok\n    ngrokToken = '填写ngrok的token'\n    conf.get_default().auth_token = ngrokToken\n    conf.get_default().monitor_thread = False\n    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n    if len(ssh_tunnels) == 0:\n        ssh_tunnel = ngrok.connect(8188)\n        print('address：'+ssh_tunnel.public_url)\n    else:\n        print('address：'+ssh_tunnels[0].public_url)\n    \n!npm install -g localtunnel\n\nimport subprocess\nimport threading\nimport time\nimport socket\nimport urllib.request\n\ndef iframe_thread(port):\n    while True:\n        time.sleep(0.5)\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex(('127.0.0.1', port))\n        if result == 0:\n            break\n        sock.close()\n        print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n        print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n        p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n\n\nthreading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n!python main.py","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:04:28.877587Z","iopub.execute_input":"2023-07-08T05:04:28.878011Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[K\u001b[?25hm##################\u001b[0m) ⠋ reify:yargs: \u001b[32;40mtiming\u001b[0m \u001b[35mreifyNode:node_modules/localtunnel/n\u001b[0m\u001b[Kocaltu\u001b[0m\u001b[K\u001b[KK\nchanged 22 packages in 1s\n\n3 packages are looking for funding\n  run `npm fund` for details\n\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\n\nThe password/enpoint ip for localtunnel is: 35.193.232.166\nyour url is: https://brave-tips-enjoy.loca.lt\nTotal VRAM 15110 MB, total RAM 16007 MB\nxformers version: 0.0.20\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nSet vram state to: NORMAL_VRAM\nDevice: cuda:0 Tesla T4\nUsing xformers cross attention\n/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\nStarting server\n\nTo see the GUI go to: http://127.0.0.1:8188\ngot prompt\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\nSetting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\nv_prediction False\nadm 2816\ngot prompt\nmaking attention of type 'vanilla-xformers' with 512 in_channels\nbuilding MemoryEfficientAttnBlock with 512 in_channels...\nWorking with z of shape (1, 4, 32, 32) = 4096 dimensions.\nmaking attention of type 'vanilla-xformers' with 512 in_channels\nbuilding MemoryEfficientAttnBlock with 512 in_channels...\nmissing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\nleft over keys: dict_keys(['denoiser.log_sigmas', 'denoiser.sigmas'])\ntorch.Size([1, 1280]) 512 512 0 0 512 512\ntorch.Size([1, 1280]) 512 512 0 0 512 512\n100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.13it/s]\nPrompt executed in 101.00 seconds\nPrompt executed in 0.00 seconds\n","output_type":"stream"}]}]}
